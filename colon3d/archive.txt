

## setup for depth model

Download and extract [EndoSfMLearner.zip](https://drive.google.com/file/d/1COFJcsqxZTSGODhKCRu1nrfzPK_roAKk/view?usp=share_link) to the folder colond3d/pretrained

## How to import data from the Unity simulator

Run `import_froms_sim.py` with the following required arguments:

* --sim_out_path: path to the simulator output (the folder containing the `Sequence_XXX` folders)
* --fps: frame retain Hz of the output videos, if 0 the frame rate will be extracted from the settings file

 Output description:

* The camera_motion.csv file contains: pos_x, pos_y and pos_z coordinates of the camera center in the world space in millimeters,
  and the [quaternion representations](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation) of rotation from world space to the camera  space (quat_x, quat_y, quat_z, quat_w).
* the optical center of the first frame camera is used as the origin point of the world space.
* Note that Unity uses a left-handed coordinate system (see [https://github.com/zsustc/colon_reconstruction_dataset](https://github.com/zsustc/colon_reconstruction_dataset))


# --------------------------------------------------------------------------------------------------------------------

# def apply_pose_transform(prev_poses: torch.Tensor, egomotions: torch.Tensor):
#     """Change the camera pose by applying the 6DOF egomotion
#     Note: the depth network is trained to predict the transformation to apply to points in frame t-1 to get points in frame t
#     The corresponding camera pose change (egomotion) is the inverse of that transformation, so we apply the inverse transformation
#     - therefore
#     see https://source.corp.google.com/piper///depot/google3/medical/endoscopy/experimental/unsupervised_vo/loss.py;l=327;bpv=1;bpt=1;rcl=517284693, line 386
#     Args:
#         prev_poses (torch.Tensor): [n_cam x 7] each row is the pre-transform camera pose (x, y, z, q0, qx, qy, qz).
#         egomotions (torch.Tensor): [n_cam x 7] each row is the camera egomotion (pose change) (x, y, z, q0, qx, qy, qz).
#     """
#     trans_vecs = egomotions[:, :3]  # translation vectors
#     rot_vecs = egomotions[:, 3:]  # unit-quaternion of the rotations
#     inv_rot_vecs = quaternion_invert(rot_vecs)  #  inverse rotation
#     prev_rot = prev_poses[:, 3:]  # unit-quaternion of the prev cam orientations
#     prev_loc = prev_poses[:, :3]  # prev cam positions
#     # since we are dealing with a change of pose, we need to invert the rotation and translation
#     new_loc = prev_loc - trans_vecs
#     new_rot = quaternion_raw_multiply(prev_rot, inv_rot_vecs)
#     new_cam_poses = torch.cat((new_loc, new_rot), dim=1)
#     return new_cam_poses






  #### create the objects that are static in the scene
    static_objs_list = []
    max_dist = -1
    # plot the tracks\polyps (static points)
    for track_id, track_p3d in tracks_p3d_world.items():
        track_p3d = track_p3d.numpy(force=True)
        #  plot the center KP of the track in red
        static_objs_list.append(
            go.Scatter3d(
                x=[track_p3d[0, 0]],
                y=[track_p3d[0, 1]],
                z=[track_p3d[0, 2]],
                mode="markers",
                marker=dict(size=3, color="red", opacity=0.4),
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=f"track_id: {track_id}",
                name=f"Track #{track_id} Center",
            )
        )
        # plot the rest of the track KPs in orange
        static_objs_list.append(
            go.Scatter3d(
                x=track_p3d[1:, 0],
                y=track_p3d[1:, 1],
                z=track_p3d[1:, 2],
                mode="markers",
                marker=dict(size=3, color="orange", opacity=0.4),
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=f"track_id: {track_id}",
                showlegend=False,
                name=f"Track #{track_id} KPs",
            )
        )
        # find the maximum distance of the camera from a track point
        max_dist = max(max_dist, np.sqrt(np.square(track_p3d[0, :] - cam_loc).sum(axis=1).max()))
# --------------------------------------------------------------------------------------------------------------------


def plot_fov_cone_camera(
    max_dist: float = 1000,
    fov_deg: float = 120,
    trans=None,
    rot=None,
    visible=True,
    fov_name="Cam. FOV",
    tip_name="Cam. Pos.",
):
    """Returns a 3D objects that represents a camera cone in normalized coordinates.
    sets the cone tip at (0,0,0) and looking at direction (0, 0, 1)."""
    z_depth = min(max_dist, 1000)
    n_circle_points = 360
    # index zero is (0,0,0) the rest are on a circle at z=z_depth, with radius = z_depth * tan(FOV/2)
    points = np.zeros((n_circle_points + 1, 3))
    points[1:, 2] = z_depth  # set z values for all points besides the tip
    theta = np.linspace(0, 2 * np.pi, num=n_circle_points)
    fov_rad = np.deg2rad(fov_deg)
    radius = z_depth * np.tan(fov_rad / 2)
    # set x,y values for all points besides the tip
    points[1:, 0] = radius * np.cos(theta)
    points[1:, 1] = radius * np.sin(theta)
    if trans is None:
        trans = np.array([0, 0, 0])
    if rot is None:
        rot = get_identity_quaternion_np()
    # translate and rotate:
    points = rotate_np(points, rot) + trans
    cam_cone = go.Mesh3d(
        x=points[:, 0],
        y=points[:, 1],
        z=points[:, 2],
        opacity=0.3,
        color="lightpink",
        visible=visible,
        name=tip_name,
        showlegend=False,
    )
    tip_point = go.Scatter3d(
        mode="markers",
        x=[points[0, 0]],
        y=[points[0, 1]],
        z=[points[0, 2]],
        marker=dict(color="blue", size=4, opacity=0.6),
        visible=visible,
        name=fov_name,
    )
    fig_data = [cam_cone, tip_point]
    # TODO: add camera X,Y,Z axis

    return fig_data


# --------------------------------------------------------------------------------------------------------------------

     # take only frames in the distance limit
    dist_sqr = np.square(cam_loc - cam_loc[0]).sum(axis=1)
    is_valid = dist_sqr < dist_lim**2
    if np.sum(is_valid) == 0:
        print("No frames in the distance limit")
        return
    frame_inds = frame_inds[is_valid]


 frame_inds = frame_inds[is_valid]
        labels = [
            f"Frame: {i_frame}, {(i_frame / fps):.1f} [sec], ({x[i]:.2f},{y[i]:.2f},{z[i]:.2f})"
            for i, i_frame in enumerate(frame_inds)
        ]
# --------------------------------------------------------------------------------------------------------------------


def plot_3d_scene_in_camera_sys(
    track_est_3d_cam,
    video_loader: VideoLoader,
    detections_tracker: DetectionsTracker,
    track_id,
    start_frame,
    stop_frame,
    show_fig=False,
    save_path=None,
    dist_lim=5000,
):
    """
    Plot the 3D scene in the camera coordinate system
    Args:
        track_est_3d_cam: 3D track in the camera coordinate system
        video_loader: video loader
        detections_tracker: detections tracker
        track_id: track id
        start_frame: start frame
        stop_frame: stop frame
        show_fig: show figure
        save_path: save path
        dist_lim: maximal distance to plot (units: mm).
    """
    # TODO: transform points to fisheye camera  and show the camera cone (radius = min_axis)
    frame_inds = np.arange(start_frame, stop_frame)
    is_track_in_view = detections_tracker.is_track_in_alg_view(track_id, frame_inds)
    fps = video_loader.fps
    track_est_3d_cam = track_est_3d_cam.numpy(force=True)
    x = track_est_3d_cam[start_frame:stop_frame, 0]
    y = track_est_3d_cam[start_frame:stop_frame, 1]
    z = track_est_3d_cam[start_frame:stop_frame, 2]
    dist_sqr = x**2 + y**2 + z**2
    if dist_lim is not None:
        is_valid = dist_sqr < dist_lim**2
    if np.sum(is_valid) == 0:
        print("No frames in the distance limit")
        return
    x = x[is_valid]
    y = y[is_valid]
    z = z[is_valid]
    max_dist = np.sqrt(np.max(dist_sqr[is_valid]))
    is_track_in_view = is_track_in_view[is_valid]
    is_unseen = np.logical_not(is_track_in_view)
    frame_inds = frame_inds[is_valid]
    labels = [
        f"Frame: {i_frame}, {(i_frame / fps):.1f} [sec], ({x[i]:.2f},{y[i]:.2f},{z[i]:.2f})"
        for i, i_frame in enumerate(frame_inds)
    ]
    # Plot all 3d points
    scatter_all = go.Scatter3d(
        x=x,
        y=y,
        z=z,
        hovertemplate="<b>%{text}</b><extra></extra>",
        text=labels,
        mode="markers",
        marker=dict(size=3, color=frame_inds, colorscale="Turbo", opacity=0.8),
    )
    # Plot "out-of-view" 3d points
    scatter_out_of_view = go.Scatter3d(
        x=x[is_unseen],
        y=y[is_unseen],
        z=z[is_unseen],
        hovertemplate="<b>%{text}</b><extra></extra>",
        text=[labels[i] for i, _ in enumerate(frame_inds) if is_unseen[i]],
        mode="markers",
        marker=dict(size=3, color="black", opacity=0.8),
    )
    cam_fig_data = plot_fov_cone_camera(
        max_dist=max_dist, fov_deg=video_loader.alg_fov_deg
    )
    fig = go.Figure(data=[scatter_all, scatter_out_of_view, *cam_fig_data])
    fig.update_layout(
        scene=dict(xaxis_title="X [mm]", yaxis_title="Y [mm]", zaxis_title="Z [mm]"),
        title=f"camera view: object center (black=out_of_view), max_dist={max_dist:1.3g} [mm]",
    )
    if save_path:
        file_path = save_path / f"camera_view_{track_id}.html"
        fig.write_html(file_path)
        print("Saved figure at ", file_path)
    if show_fig:
        fig.show()

# --------------------------------------------------------------------------------------------------------------------

        #    mode="lines+markers",
        # hovertemplate="<b>%{text}</b><extra></extra>",
        # text=[f"Frame: {i_frame}" for i_frame in frame_inds[:i_step]],
        # marker=dict(
        #     size=3,
        #     opacity=0.7,
        #     color=color_map(t_vec[:i_step]),
        # ),  # , color=frame_inds[:i_step], colorscale="Viridis"


def plot_3d_scene_in_world_sys(
    cam_poses,
    start_frame,
    stop_frame,
    tracks_p3d_world=None,
    save_path=None,
    show_fig=False,
    dist_lim=5000,
):
    """Plots the 3D scene in world system, with the camera trajectory and the 3D points of the track.
    Args:
        cam_poses: camera poses in world system
        start_frame: start frame
        stop_frame: stop frame
        track_p3d_world: 3D point of the track in world system (assumed static - single point for all frames)
        save_path: path to save the figure
        show_fig: show figure
        dist_lim: maximum distance from the first frame to plot
    """
    cam_poses = cam_poses.numpy(force=True)
    cam_loc = cam_poses[:, :3]
    frame_inds = np.arange(start_frame, stop_frame, dtype=int)

    # filter the frames that are too far from the first frame
    dist_sqr = np.square(cam_loc - cam_loc[0]).sum(axis=1)
    if dist_lim is not None:
        is_valid = dist_sqr < dist_lim**2
    if np.sum(is_valid) == 0:
        print("No frames in the distance limit")
        return
    frame_inds = frame_inds[is_valid]

    # Initialize the figure
    fig = go.Figure()
    fig.update_layout(
        scene=dict(xaxis_title="X [mm]", yaxis_title="Y [mm]", zaxis_title="Z [mm]"),
        title="World View",
    )
    i_obj = 0 # index of the object in the figure
    # Add the static tracks (polyps)
    static_obj_ids = []
    for track_id, track_p3d in tracks_p3d_world.items():
        track_p3d = track_p3d.numpy(force=True)
        n_track_points = track_p3d.shape[0]
         #  the center KP of the track in red
        fig.add_trace(
            go.Scatter3d(
                x=[track_p3d[0, 0]],
                y=[track_p3d[0, 1]],
                z=[track_p3d[0, 2]],
                mode="markers",
                marker=dict(size=5, color="red", opacity=0.4),
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=f"track_id: {track_id}",
            )
        )
        static_obj_ids.append(i_obj)
        i_obj += 1
        # rest of the track KPs in orange
        fig.add_trace(
            go.Scatter3d(
                x=track_p3d[1:n_track_points, 0],
                y=track_p3d[1:n_track_points, 1],
                z=track_p3d[1:n_track_points, 2],
                mode="markers",
                marker=dict(size=5, color="orange", opacity=0.4),
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=f"track_id: {track_id}",
            )
        )
        static_obj_ids.append(i_obj)
        i_obj += 1

    # Add per-frame objects, one for each slider step
    dyn_obj_ids_in_step = [[] for i in range(len(frame_inds))]
    for i_step, i_frame in enumerate(frame_inds):
        cam_loc = cam_poses[i_frame, 0:3]
        cam_rot = cam_poses[i_frame, 3:7]
        label = f"Frame: {i_frame}, ({cam_loc[0]:1.2f},{cam_loc[1]:.2f},{cam_loc[2]:.2f})"
        # add the camera 3D position in current frame
        fig.add_trace(
            go.Scatter3d(
                x=[cam_loc[0]],
                y=[cam_loc[1]],
                z=[cam_loc[2]],
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=label,
                mode="markers",
                marker=dict(size=5, color=frame_inds, colorscale="Viridis", opacity=0.8),
            )
        )
        dyn_obj_ids_in_step[i_step].append(i_obj)
        i_obj += 1
        # add the camera FOV cone in current frame
        # fig.add_trace(



    n_objs = len(fig.data)
    # Create and add slider
    steps = []
    for i_step, i_frame in enumerate(frame_inds):
        cur_obj_ids = static_obj_ids + dyn_obj_ids_in_step[i_step]
        # set the visibility of the objects in the current step
        is_objs_vis = [False] * n_objs
        cur_vis_obj_ids = static_obj_ids + dyn_obj_ids_in_step[i_step]
        for i_obj in cur_vis_obj_ids:
            is_objs_vis[i_obj] = True
        step = dict(
            method="update",
            args=[{"visible": is_objs_vis},
                {"title": "Slider switched to step: " + str(i_step)}],  # layout attribute
        )
        step["args"][0]["visible"][i_step] = True  # Toggle i'th trace to "visible"
        steps.append(step)

    sliders = [dict(
        active=10,
        currentvalue={"prefix": "Frame: "},
        pad={"t": 50},
        steps=steps
    )]

    fig.update_layout(
        sliders=sliders
    )

    if save_path:
        file_path = save_path / f"world_view.html"
        fig.write_html(file_path)
        print("Saved figure at ", file_path)
    if show_fig:
        fig.show()

    # Create the 3D scatter plot.
    for i_frame in range(n_frames):
        fig.add_trace(
            go.Scatter3d(
                x=cam_loc[i_frame, 0],
                y=cam_loc[i_frame, 1],
                z=cam_loc[i_frame, 2],
                mode="markers",
                marker=dict(size=5, color=t_vec, colorscale="Viridis", opacity=0.8),
                name="Trajectory",
            )
        )

    # Add the time step slider.
    is_visible = np.zeros((n_frames, n_frames), dtype=bool)
    for i in range(n_frames):
        is_visible[i, i] = True

    # Define step as a list
    steps = []
    # We loop through the length of the dataset and try to define steps at each interval
    for i in range(len(fig.data)):
        # Defining step
        step = dict(
            # Using restyle method as we are changing underlying data
            method="restyle",
            # Setting all traces to invisible mode - visibility set to false
            args=["visible", [False] * len(fig.data)],
        )
        step["args"][1][i] = True  # Toggle i'th trace to "visible"
        # Append step to the 'steps' list
        steps.append(step)

    # Defining the slider
    sliders = [
        dict(
            active=10,
            currentvalue={"prefix": "Frequency: "},
            pad={"t": 50},
            # Assigning steps of the slider
            steps=steps,
        )
    ]
    fig.update_layout(
        sliders=sliders,
        title=f"Time: {t_vec[0]:.2f}",
        scene=dict(
            xaxis=dict(title="X"),
            yaxis=dict(title="Y"),
            zaxis=dict(title="Z"),
        ),
    )

*****************




def plot_3d_scene_in_world_sys(
    cam_poses,
    start_frame,
    stop_frame,
    tracks_p3d_world=None,
    save_path=None,
    show_fig=False,
    dist_lim=5000,
):
    """Plots the 3D scene in world system, with the camera trajectory and the 3D points of the track.
    Args:
        cam_poses: camera poses in world system
        start_frame: start frame
        stop_frame: stop frame
        track_p3d_world: 3D point of the track in world system (assumed static - single point for all frames)
        save_path: path to save the figure
        show_fig: show figure
        dist_lim: maximum distance from the first frame to plot
    """
    cam_poses = cam_poses.numpy(force=True)
    cam_loc = cam_poses[:, :3]
    frame_inds = np.arange(start_frame, stop_frame, dtype=int)

    # filter the frames that are too far from the first frame
    dist_sqr = np.square(cam_loc - cam_loc[0]).sum(axis=1)
    if dist_lim is not None:
        is_valid = dist_sqr < dist_lim**2
    if np.sum(is_valid) == 0:
        print("No frames in the distance limit")
        return
    frame_inds = frame_inds[is_valid]

    # Initialize the figure
    fig = go.Figure()
    fig.update_layout(
        scene=dict(xaxis_title="X [mm]", yaxis_title="Y [mm]", zaxis_title="Z [mm]"),
        title="World View",
    )
    i_obj = 0 # index of the object in the figure
    # Add the static tracks (polyps)
    static_obj_ids = []
    for track_id, track_p3d in tracks_p3d_world.items():
        track_p3d = track_p3d.numpy(force=True)
        n_track_points = track_p3d.shape[0]
         #  the center KP of the track in red
        fig.add_trace(
            go.Scatter3d(
                x=[track_p3d[0, 0]],
                y=[track_p3d[0, 1]],
                z=[track_p3d[0, 2]],
                mode="markers",
                marker=dict(size=5, color="red", opacity=0.4),
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=f"track_id: {track_id}",
            )
        )
        static_obj_ids.append(i_obj)
        i_obj += 1
        # rest of the track KPs in orange
        fig.add_trace(
            go.Scatter3d(
                x=track_p3d[1:n_track_points, 0],
                y=track_p3d[1:n_track_points, 1],
                z=track_p3d[1:n_track_points, 2],
                mode="markers",
                marker=dict(size=5, color="orange", opacity=0.4),
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=f"track_id: {track_id}",
            )
        )
        static_obj_ids.append(i_obj)
        i_obj += 1

    # Add per-frame objects, one for each slider step
    dyn_obj_ids_in_step = [[] for i in range(len(frame_inds))]
    for i_step, i_frame in enumerate(frame_inds):
        cam_loc = cam_poses[i_frame, 0:3]
        cam_rot = cam_poses[i_frame, 3:7]
        label = f"Frame: {i_frame}, ({cam_loc[0]:1.2f},{cam_loc[1]:.2f},{cam_loc[2]:.2f})"
        # add the camera 3D position in current frame
        fig.add_trace(
            go.Scatter3d(
                x=[cam_loc[0]],
                y=[cam_loc[1]],
                z=[cam_loc[2]],
                hovertemplate="<b>%{text}</b><extra></extra>",
                text=label,
                mode="markers",
                marker=dict(size=5, color=frame_inds, colorscale="Viridis", opacity=0.8),
            )
        )
        dyn_obj_ids_in_step[i_step].append(i_obj)
        i_obj += 1
        # add the camera FOV cone in current frame
        # fig.add_trace(



    n_objs = len(fig.data)
    # Create and add slider
    steps = []
    for i_step, i_frame in enumerate(frame_inds):
        cur_obj_ids = static_obj_ids + dyn_obj_ids_in_step[i_step]
        # set the visibility of the objects in the current step
        is_objs_vis = [False] * n_objs
        cur_vis_obj_ids = static_obj_ids + dyn_obj_ids_in_step[i_step]
        for i_obj in cur_vis_obj_ids:
            is_objs_vis[i_obj] = True
        step = dict(
            method="update",
            args=[{"visible": is_objs_vis},
                {"title": "Slider switched to step: " + str(i_step)}],  # layout attribute
        )
        step["args"][0]["visible"][i_step] = True  # Toggle i'th trace to "visible"
        steps.append(step)

    sliders = [dict(
        active=10,
        currentvalue={"prefix": "Frame: "},
        pad={"t": 50},
        steps=steps
    )]

    fig.update_layout(
        sliders=sliders
    )

    if save_path:
        file_path = save_path / f"world_view.html"
        fig.write_html(file_path)
        print("Saved figure at ", file_path)
    if show_fig:
        fig.show()



# --------------------------------------------------------------------------------------------------------------------


def plot_cam_animated_path_in_world_sys(
    cam_poses,
    start_frame,
    stop_frame,
    tracks_p3d_world=None,
    save_path=None,
    show_fig=False,
    dist_lim=5000,
    fps=30,
):
    """Plots the 3D scene in world system, with the camera trajectory and the 3D points of the track.
    Args:
        cam_poses: camera poses in world system
        start_frame: start frame
        stop_frame: stop frame
        save_path: path to save the figure
        show_fig: show figure
        dist_lim: maximum distance from the first frame to plot
    """
    plt.rcParams["animation.html"] = "jshtml"
    frame_inds = np.arange(start_frame, stop_frame, dtype=int)
    cam_loc = cam_poses[frame_inds, 0:3]
    cam_rot = cam_poses[frame_inds, 3:7]
    cam_loc = cam_loc.numpy(force=True)
    cam_rot = cam_rot.numpy(force=True)
    x_cam = cam_loc[:, 0]
    y_cam = cam_loc[:, 1]
    z_cam = cam_loc[:, 2]
    dist_sqr = (x_cam - x_cam[0]) ** 2 + (y_cam - y_cam[0]) ** 2 + (z_cam - z_cam[0]) ** 2
    if dist_lim is not None:
        is_valid = dist_sqr < dist_lim**2
    if np.sum(is_valid) == 0:
        print("No frames in the distance limit")
        return
    # filter out invalid frames
    x_cam = x_cam[is_valid]
    y_cam = y_cam[is_valid]
    z_cam = z_cam[is_valid]
    cam_rot = cam_rot[is_valid]
    frame_inds = frame_inds[is_valid]
    n_frames = len(frame_inds)
    labels = [
        f"Frame: {i_frame}, ({x_cam[i]:.2f},{y_cam[i]:.2f},{z_cam[i]:.2f})" for i, i_frame in enumerate(frame_inds)
    ]

    # plot elements for the tracks (static points)
    static_data = []
    for track_id, track_p3d in tracks_p3d_world.items():
        #  the center KP of the track
        static_data.append(
            go.Scatter3d(
                x=[track_p3d[0, 0]],
                y=[track_p3d[0, 1]],
                z=[track_p3d[0, 2]],
                mode="markers",
                marker=dict(size=5, color="red", opacity=0.4),
            )
        )
        #  rest of the track KPs
        static_data.append(
            go.Scatter3d(
                x=track_p3d[1:, 0],
                y=track_p3d[1:, 1],
                z=track_p3d[1:, 2],
                mode="markers",
                marker=dict(size=5, color="orange", opacity=0.4),
            )
        )

    # create frames
    frames_data = []
    for i_frame in range(n_frames):
        cur_frame_data = []
        cur_frame_data.append(
            go.Scatter3d(
                x=x_cam[:i_frame],
                y=y_cam[:i_frame],
                z=z_cam[:i_frame],
                hovertemplate="<b>%{text}</b><extra></extra>",
                mode="markers",
                text=labels[:i_frame],
                marker=dict(size=5, color=frame_inds, colorscale="Viridis", opacity=0.8),
            )
        )
        cur_frame_data += static_data
        frames_data.append(go.Frame(data=cur_frame_data))

    # Create figure
    fig = go.Figure(data=static_data, frames=frames_data)

    fig.update_layout(
        scene=dict(
            xaxis=dict(range=[x_cam.min() - 2, x_cam.max() + 2], autorange=False, title="X [mm]"),
            yaxis=dict(range=[y_cam.min() - 2, y_cam.max() + 2], autorange=False, title="Y [mm]"),
            zaxis=dict(range=[z_cam.min() - 2, z_cam.max() + 2], autorange=False, title="Z [mm]"),
        ),
        margin=dict(r=0, l=0, b=0, t=0),
    )

    # add buttons to play/pause the animation
    fig.update_layout(
        updatemenus=[
            dict(
                type="buttons",
                showactive=False,
                buttons=[
                    dict(label="Play", method="animate", args=[None, {"frame": {"duration": 1 / fps, "redraw": True}}]),
                    dict(
                        label="Play X10",
                        method="animate",
                        args=[None, {"frame": {"duration": 10 / fps, "redraw": True}}],
                    ),
                ],
            )
        ]
    )

    if save_path:
        file_path = save_path / f"world_view_animated.html"
        # save the animation as an HTML5 vide
        fig.write_html(file_path, auto_play=False, full_html=False)
        print("Saved figure at ", file_path)
    if show_fig:
        fig.show()


# --------------------------------------------------------------------------------------------------------------------



       max_polyp_size_mm = 50 # maximum size of a polyp in mm (limits the estimated 3D spatial size of the polyp bounding box)
    track_kps_lim_dist_penalty_weight = 0.1  # default weighting for the penalty term of violating the maximal distance between the track keypoints in 3D world coordinates


       # max_3d_guess_err = 500  # (units: mm)


    # add a log-barrier penalties for violating the spatial size of a polyp (in the 3d world)

    for track_id, cur_track_p3d_inds in tracks_p3d_inds.items():
        track_p3d = points_3d[cur_track_p3d_inds, :]


        # find the limit on the distance between a track's (polyp's) keypoints in the 3D space
    detect_bb_kps_ratios = alg_prm.detect_bb_kps_ratios
    max_polyp_size_mm = alg_prm.max_polyp_size_mm
    n_track_kps = len(detect_bb_kps_ratios)
    track_kps_lim_dists = dict()
    for i in range(n_track_kps - 1):
        for j in range(i + 1, n_track_kps):
            track_kps_lim_dist  = max_polyp_size_mm * np.linalg.norm(
                detect_bb_kps_ratios[i, :] - detect_bb_kps_ratios[j, :]
            )
            constraints[f"track_kps_lim_dist_{i}_{j}"] = dict(lim_type="upper", lim=track_kps_lim_dist, weight=alg_prm.track_kps_lim_dist_penalty_weight)


    #  weighting for the log-barrier penalty terms in the bundle adjustment cost function:


    # points_guess_err_penalty_weight: float = 0. # weights for penalties for violating the bounds on the 3d world point guesses based on the depth estimation error

    # # add a log-barrier penalties for violating the bounds on the 3d world point guesses based on the depth estimation error
    # penalty_points_guess_err_lim =  0
    # if penalizer.constraints["points_guess_err_lim"]["weight"] > 0:
    #     guess_errors = torch.linalg.norm(points_3d_guess[p3d_opt_flag] - points_3d_opt, dim=1)
    #     for guess_error in guess_errors:
    #         penalty_points_guess_err_lim += penalizer.get_penalty(constraint_name="points_guess_err_lim", val=guess_error)



# cam_pose_speed estimates of the camera pose speed (in world system) - used to guess the current camera pose
#  the speed estimates the rate of change of each cooridnate per frame
cam_pose_speed = np.zeros((6))

        # update the camera pose speed estimate
        # if i_frame > 1:
        #     last_pose_change = cam_poses[-1] - cam_poses[-2]
        #     cam_pose_speed = last_pose_change * alg_prm.cam_speed_change_factor + cam_pose_speed * (
        #         1 - alg_prm.cam_speed_change_factor
        #     )
        #     cam_pose_speed[:3] = np.clip(cam_pose_speed[:3], -max_angle_delta, max_angle_delta)
        #     cam_pose_speed[3:6] = np.clip(cam_pose_speed[3:6], -max_pos_delta, max_pos_delta)


    # z_arrow_scale = 0.05
    # z_arrow_base = np.array([0.5 * frame_width, 10], dtype=int)

        # z_arrow_tip = z_arrow_base.copy()
        # z_arrow_tip[0] = z_arrow_base[0] + obj_z_from_im_plane[i] * z_arrow_scale
        # z_arrow_tip = clip_arrow(
        #     z_arrow_tip,
        #     z_arrow_base,
        #     max_arrow_len,
        #     lower_lim_x=0,
        #     lower_lim_y=0,
        #     upper_lim_x=frame_width,
        #     upper_lim_y=frame_height,
        # ).astype(int)


        # vis_frame = cv2.arrowedLine(
        #     vis_frame,
        #     (z_arrow_base[0], z_arrow_base[1]),
        #     (z_arrow_tip[0], z_arrow_tip[1]),
        #     color=z_arrow_color,
        #     thickness=8,
        # )




         "x_min_alg": x_min_alg,
        "x_max_alg": x_max_alg,
        "y_min_alg": y_min_alg,
        "y_max_alg": y_max_alg,
        "cx_in_alg_view": new_im_size / 2,
        "cy_in_alg_view": new_im_size / 2,
        "cx_in_full_view": x_min_alg + new_im_size / 2,
        "cy_in_full_view": y_min_alg + new_im_size / 2,
        "alg_view_radius_pix": new_im_size / 2,
        "alg_fov_deg": alg_fov_deg,):
        crop_for_alg_info = self.crop_for_alg_info
        x_min_alg = crop_for_alg_info["x_min_alg"]
        x_max_alg = crop_for_alg_info["x_max_alg"]
        y_min_alg = crop_for_alg_info["y_min_alg"]
        y_max_alg = crop_for_alg_info["y_max_alg"]
        frame = frame[y_min_alg:y_max_alg, x_min_alg:x_max_alg]
        # set the image to black outside the FOV circle:
        cx = round(crop_for_alg_info["cx_in_alg_view"])
        cy = round(crop_for_alg_info["cy_in_alg_view"])
        radius = round(crop_for_alg_info["alg_view_radius_pix"])





    def create_alg_view_cam_info(self):
        # find the FOV angle of the the original image (in the minimal direction)
        x_size = min(cx_raw, width - cx_raw) * 2
        y_size = min(cy_raw, height - cy_raw) * 2
        orig_fov_deg_x = np.rad2deg(np.arctan2(x_size / 2, fx)) * 2
        orig_fov_deg_y = np.rad2deg(np.arctan2(y_size / 2, fy)) * 2
        orig_fov_deg = min(orig_fov_deg_x, orig_fov_deg_y)
        alg_fov_deg = orig_fov_deg * alg_fov_ratio
        effective_alg_fov_ratio = np.tan(np.deg2rad(alg_fov_deg) / 2) / np.tan(np.deg2rad(orig_fov_deg) / 2)
        # the new image will be of squared dimensions, where outside the FOV circle, the image is black:
        im_size = min(x_size, y_size)
        alg_view_im_size = int(im_size * effective_alg_fov_ratio)
        alg_view_radius = alg_view_im_size / 2 # the radius of the circle that is visible to the algorithm (around the optical center)
        print(
            f"The SLAM algorithm will get only a FOV of {alg_fov_deg:.2f} [deg] out of the original FOV of {orig_fov_deg:.2f} [deg], the rest is only used for validation"
        )
        print(
            f"The algorithm will uses  a cropped image of [{alg_view_im_size} X {alg_view_im_size}] from the original image of [{height} X {width}]"
        )
        # the coordnates in the full image tha the algorithm views:


        max_focal_length = max(orig_cam_info.fx, orig_cam_info.fy)
        self.fov_deg_new = np.rad2deg(2 * np.arctan(self.view_radius / max_focal_length))



class ImageUndistorter:
    """Class to undistort fisheye camera images to rectilinear images."""

    def __init__(self, cam_K_mat, cam_distort_param):
        self.frame_size_undistort = {"width": 1000, "height": 1000}
        self.FOV_deg_undistort = 120
        self.K_undistort = get_desired_camera_matrix(self.frame_size_undistort, self.FOV_deg_undistort)
        # create undistortion map for images:
        self.undistort_map1, self.undistort_map2 = cv2.fisheye.initUndistortRectifyMap(
            K=cam_K_mat,
            D=cam_distort_param,
            R=np.eye(3),
            P=self.K_undistort,
            size=(self.frame_size_undistort["height"], self.frame_size_undistort["width"]),
            m1type=cv2.CV_32F,
        )

    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    def undistort_img(self, img):
        return cv2.remap(
            img,
            self.undistort_map1,
            self.undistort_map2,
            interpolation=cv2.INTER_AREA,
            borderMode=cv2.BORDER_CONSTANT,
        )


# --------------------------------------------------------------------------------------------------------------------


def get_desired_camera_matrix(frame_size_undistort, deisred_FOV_deg):
    # Set the desired camera matrix
    # h,w are the desired image height and width [pixels]
    # FOV is in [deg]
    frame_width = frame_size_undistort["width"]
    frame_height = frame_size_undistort["height"]
    focal_len = frame_width / (2 * math.tan(math.pi * (deisred_FOV_deg / 2) / 180))
    K_undistort = np.array([[focal_len, 0, frame_width / 2], [0, focal_len, frame_height / 2], [0, 0, 1]])
    return K_undistort


        # create undistortion map for images (to help with the coordinate transformation):
        self.image_undistorter = ImageUndistorter(
            self.cam_K_mat,
            self.cam_distort_param,
        )
***********
    elif opt_method == "trust_constr":
        bounds = {"ub": +upper_bound,  "lb": -upper_bound}
        result = torchmin.minimize_constr(
            f=sse_cost_func,
            x0=x0,
            constr=None,
            bounds=bounds,
            max_iter=3,
            tol=1e-5,
            callback=None,
            disp=2,
        )



    # set the objective function
    def sse_cost_func(x) -> torch.Tensor:
        # The Sum of Squared Errors Loss function
        tot_loss = torch.mean(torch.square(losses_vector(x)))
        return tot_loss

    def sae_cost_func(x) -> torch.Tensor:
        # The Sum of Absolute Errors Loss function
        tot_loss = torch.mean(torch.abs(losses_vector(x)))
        return tot_loss


    elif opt_method == "least_squares":
        result = torchmin.least_squares(
            fun=losses_vector,
            x0=x0,
            bounds=None,
            method="trf",  # "dogbox" or "trf"
            ftol=1e-5,
            xtol=1e-7,
            x_scale=1.0,  # "jac"
            tr_solver="lsmr",
            tr_options=None,
            max_nfev=25,
            verbose=verbose,
        )


    # # set the bounds of the optimization parameters:
    # upper_bound = torch.zeros_like(input=x0)
    # lower_bound =  torch.zeros_like(input=x0)
    # frames_opt_inds = np.where(frames_opt_flag)[0].tolist()
    # for i, frame_idx in enumerate(frames_opt_inds):
    #     # set the upper bound of the i-th optimized camera pose to be the max delta from its estimated pose in the previous frame
    #     prev_frame_idx = frame_idx - 1
    #     prev_cam_pose = cam_poses[prev_frame_idx]
    #     lower_bound[i * 6 : i * 6 + 3] =  prev_cam_pose[3:6] - max_angle_delta
    #     upper_bound[i * 6 : i * 6 + 3] =  prev_cam_pose[3:6] + max_angle_delta
    #     lower_bound[i * 6 + 3 : i * 6 + 6] = prev_cam_pose[:3] - max_pos_delta
    #     upper_bound[i * 6 + 3 : i * 6 + 6] =  prev_cam_pose[:3] + max_pos_delta

    # # indexes of the 3d points that are optimized
    # p3d_opt_inds = np.where(p3d_opt_flag)[0].tolist()
    # start_idx = n_cam_poses_opt * 6
    # for i, p3d_idx in enumerate(p3d_opt_inds):
    #     upper_bound[start_idx + i * 3 : start_idx + i * 3 + 3] =  points_3d[p3d_idx] + alg_prm.max_3d_est_err
    #     lower_bound[start_idx + i * 3 : start_idx + i * 3 + 3] =  points_3d[p3d_idx] - alg_prm.max_3d_est_err



    max_3d_est_err: float = (
        1000  # (units: mm) the maximum error in the 3d point estimation (used for optimization constraints)
    )


# def unproject_pixel_to_world(
#     points_2d: np.ndarray,
#     z_depth: torch.Tensor,
#     cam_poses: torch.Tensor,
#     camera_K: np.ndarray,
# ) -> torch.Tensor:
#     """Transforms a pixel coorinate and a given z depth to a 3D point given the camera intirnsics matrix K.
#         Assumes rectlinear image  (fisheye undistrotion was done)
#     Args:
#         points_2d: [n_points x 2]  (units: normalized image coordinates)
#         z_depth: [n_points] (units: distance units)
#         cam_poses: [n_points x 6]  (units: distance units and radians)
#     Returns:
#         points_3d: [n_points x 3]  (units: distance units)
#     """
#     translate_param = cam_poses[:, 3:6]
#     rot_param = cam_poses[:, :3]
#     z_cam_sys = z_depth
#     c_x = camera_K[0, 2]
#     c_y = camera_K[1, 2]
#     focal_len_x = camera_K[0, 0]
#     focal_len_y = camera_K[0, 0]
#     x_cam_sys = (points_2d[:, 0] - c_x) * z_depth / focal_len_x
#     y_cam_sys = (points_2d[:, 1] - c_y) * z_depth / focal_len_y
#     points_3d_cam_sys = np.stack((x_cam_sys, y_cam_sys, z_cam_sys), axis=1)
#     #  translate & rotate to world system
#     points_3d = rotate_np(points_3d_cam_sys - translate_param, -rot_param)
#     return points_3d


# -------------------------------------------------------------------------------------------------------------------


# def project_world_to_pixel(
#     cur_points_3d: torch.Tensor,
#     cur_cam_poses: torch.Tensor,
#     focal_len: torch.Tensor,
#     optical_axis_pix_coord: torch.Tensor,
# ):
#     """Convert 3-D points to 2-D by projecting onto images.
#         assumes camera prarameters set a rectlinear image transform from 3d to 2d (i.e., fisheye undistortion was done)
#     Args:
#         cur_points_3d [n_points x 3] 3d points in world coordinates (units: mm)
#         cur_cam_poses [n_points x 6] camera poses (units: radians & meters)
#         focal_len: focal length in pixels (units: mm)
#         optical_axis_pix_coord: optical axis pixel coordinate (units: radians & mm)
#     Returns:
#         [n_points x 2] pixel coordinates   (units: mm)
#     """
#     # Rotate & translate to camera system
#     eps = 1e-20
#     translate_param = cur_cam_poses[:, 3:6]  # [n_points x 3]
#     rot_param = cur_cam_poses[:, :3]  # [n_points x 3]
#     points_cam_sys = rotate(cur_points_3d, rot_param) + translate_param
#     # Prespective transform to 2d image-plane
#     z_cam_sys = points_cam_sys[:, 2]  # [n_points x 1]
#     x_wrt_axis = points_cam_sys[:, 0] * focal_len / (z_cam_sys + eps)  # [n_points x 1] [pixels]
#     y_wrt_axis = points_cam_sys[:, 1] * focal_len / (z_cam_sys + eps)  # [n_points x 1] [pixels]
#     points_2d_wrt_axis = torch.stack((x_wrt_axis, y_wrt_axis), dim=1)  # [n_points x 2]
#     points_2d = points_2d_wrt_axis + optical_axis_pix_coord
#     return points_2d

# --------------------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------------------


def plot_pyramid_camera(
    frame_width: int,
    frame_height: int,
    max_dist: float,
):
    """Retrns a 3D objects that repeesents  a camera frustum  in normalized coordinates.,
    sets the pyramid tip at (0,0,0) and looking at direction (0, 0, 1)"""
    corners_2d_nrm = torch.Tensor(
        [
            [1, -1],
            [1, 1],
            [-1, 1],
            [-1, -1],
        ]
    )
    z_depth = min(max_dist, 1000) * np.ones((4))
    # set the camera pose to be at (0,0,0) and looking at direction (0, 0, 1) :
    cam_poses = np.zeros((4, 7))
    cam_poses[:, 2] = np.pi / 2
    # transform the image corners pixel coordinates to 3D points in camea system (no ration and translation) at depth=focal_len
    corners_3d = unproject_normalized_coord_to_world(
        points_nrm=corners_2d_nrm,
        z_depth=z_depth,
        cam_poses=cam_poses,
    )
    rect = go.Scatter3d(
        x=corners_3d[:, 0],
        y=corners_3d[:, 1],
        z=corners_3d[:, 2],
        line=dict(color="green"),
        showlegend=False,
        marker=dict(size=0.0001),
    )
    focal_point = np.zeros((1, 3))
    x, y, z = np.concatenate((focal_point, corners_3d)).T
    i = [0, 0, 0, 0]
    j = [1, 2, 3, 4]
    k = [2, 3, 4, 1]
    pyramid1 = go.Mesh3d(
        x=x,
        y=y,
        z=z,
        color="lightpink",
        opacity=0.50,
        i=i,
        j=j,
        k=k,
        showlegend=False,
    )
    triangles = np.vstack((i, j, k)).T
    vertices = np.concatenate((focal_point, corners_3d))
    tri_points = np.array([vertices[i] for i in triangles.reshape(-1)])
    x, y, z = tri_points.T
    pyramid2 = go.Scatter3d(
        x=x,
        y=y,
        z=z,
        mode="lines",
        line=dict(color="blue", width=0.8),
        showlegend=False,
    )
    fig_data = [rect, pyramid1, pyramid2]
    return fig_data


def rotate(points3d: torch.Tensor, rot_vecs: torch.Tensor):
    """Rotate points by given rotation vectors. Rodrigues' rotation formula is used.
    Args:
        points3d [n_points x 3] each row is  (x, y, z) coordinates of a point to be rotated.
        rot_vecs [n_points x 4] each row is the unit-quaternion of the rotation (q0, q1, q2 , q3).
    Returns:
        [n_points x 3] rotated points.
    """
    assert points3d.shape[0] == rot_vecs.shape[0]
    eps = 1e-20





    theta = torch.linalg.norm(rot_vecs, dim=1, keepdim=True)  # [n_points x 1]
    v = rot_vecs / (theta + eps)  # [n_points x 3]
    dot = torch.sum(points3d * v, dim=1, keepdim=True)  # [n_points x 1]
    cos_theta = torch.cos(theta)  # [n_points x 1]
    sin_theta = torch.sin(theta)  # [n_points x 1]
    cross_prod = torch.cross(v, points3d)  # [n_points x 3]
    return cos_theta * points3d + sin_theta * cross_prod + dot * (1.0 - cos_theta) * v
    # if  torch.sum(torch.abs(rot1 - rot2)) < 1e-9:
    #     # if the rotations are the same, the angle between them is 0 (we don't want to get pi because of numerical errors)
    #     return torch.as_tensor(0.0).to(rot1.device)
    # theta1 = torch.linalg.norm(rot1)
    # theta2 = torch.linalg.norm(rot2)
    # r1 = rot1 / (theta1 + 1e-20)
    # r2 = rot2 / (theta2 + 1e-20)
    # rot_change = 2 * torch.arccos(
    #     torch.abs(
    #         torch.dot(r1, r2)
    #         * (
    #             torch.sin(0.5 * theta1) * torch.sin(0.5 * theta2)
    #             + torch.cos(0.5 * theta1) * torch.cos(0.5 * theta2)
    #         )
    #     )
    # )
    # the real part of the quaternion is the cosine of  0.5 times the rotation angle.
    # Note:. we used abs to avoid negative values due to optimization steps making the qunaternion non-standard

# --------------------------------------------------------------------------------------------------------------------

def up_boundary_penalty(
    val: torch.Tensor,
    upper_lim: float,
    margin_ratio: float = 0.01,
    barrier_jump: float = 1e3,
    verbose: bool = False,
) -> torch.Tensor:
    eps = 1e-20
    if verbose and val > upper_lim:
        print(f"the value val={val} exceeds the constraint upper_lim={upper_lim}, ... make sure the optimization starts from a feasible point!")
    up_margin = np.abs(upper_lim * margin_ratio)
    up_violate = val - (upper_lim - up_margin) # positive if val is above the (upper_limit - margin)
    log_up_margin = np.log(up_margin + eps)
    return up_lim_log_barrier(val, upper_lim, up_violate, log_up_margin, barrier_jump)
# --------------------------------------------------------------------------------------------------------------------


@torch.jit.script  # disable this for debugging
def up_lim_log_barrier(
    val: torch.Tensor,
    upper_lim: float,
    up_violate: float,
    log_up_margin : float,
    barrier_jump: float = 1e3,
) -> torch.Tensor:
    eps = 1e-20
    penalty_up = -torch.log(nnf.relu(upper_lim - val) + eps) + log_up_margin
    penalty_up =  torch.clamp(penalty_up, min=0, max=barrier_jump)
    penalty_up +=  barrier_jump * nnf.relu(up_violate)
    penalty_low =  torch.where(down_violate > 0, -torch.log(nnf.relu(val - lower_lim)+ eps) + np.log(low_margin + eps), 0)
    penalty_low =  torch.clamp(penalty_low, min=0, max=barrier_jump)
    penalty_low += barrier_jump * nnf.relu(down_violate)
    penalty = penalty_low + penalty_up
    return penalty



# --------------------------------------------------------------------------------------------------------------------

def low_boundary_penalty(
    val: torch.Tensor,
    lower_lim: float = -np.inf,
    margin_ratio: float = 0.01,
    barrier_jump: float = 1e3,
    verbose: bool = False,
) -> torch.Tensor:

    if verbose and val < lower_lim:
        print(f"the value val={val} is below the constraint lower_lim={lower_lim}, ... make sure the optimization starts from a feasible point!")
    low_margin = np.abs(lower_lim * margin_ratio)
    low_violate = (lower_lim + low_margin) - val # positive if val is below the (lower_limit + margin)

# --------------------------------------------------------------------------------------------------------------------


@torch.jit.script  # disable this for debugging
def low_lim_log_barrier(
    val: torch.Tensor,
    lower_lim: float = -np.inf,
    margin_ratio: float = 0.01,
    barrier_jump: float = 1e3,
) -> torch.Tensor:
    eps = 1e-20
    up_violate = val - (upper_lim - up_margin) # positive if val is above the (upper_limit - margin)
    penalty_up = torch.where(up_violate > 0, -torch.log(nnf.relu(upper_lim - val) + eps) + np.log(up_margin + eps), 0)
    penalty_up =  torch.clamp(penalty_up, min=0, max=barrier_jump)
    penalty_up +=  barrier_jump * nnf.relu(up_violate)
    down_violate = (lower_lim + low_margin) - val # positive if val is below the (lower_limit + margin)
    penalty_low =  torch.where(down_violate > 0, -torch.log(nnf.relu(val - lower_lim)+ eps) + np.log(low_margin + eps), 0)
    penalty_low =  torch.clamp(penalty_low, min=0, max=barrier_jump)
    penalty_low += barrier_jump * nnf.relu(down_violate)
    penalty = penalty_low + penalty_up
    return penalty


# --------------------------------------------------------------------------------------------------------------------
